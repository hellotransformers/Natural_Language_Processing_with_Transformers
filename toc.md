[TOC]



## 第1章 ["Hello，Transformers"](chapter1.md) [done]

介绍了Transformers并将其置于背景之中。 它还提供了一个关于 "Hugging face"生态系统的介绍。 

##  第2章，[文本分类](chapter2.md)  [done]

重点是情感分析的任务（一个常见的文本分类问题），并介绍了Trainer API。 

##  第3章，[Transformers架构剖析](chapter3.md) [done]

更深入地探讨Transformer架构，为后面的章节做准备。 

##  第4章 [多语言命名实体识别](chapter4.md) [done]

侧重于识别多语言文本中的实体的任务（一个标记分类问题）。 

## 第5章，[文本生成](chapter5.md)

探讨了Transformers模型生成文本的能力，并介绍了解码策略和度量。 

## 第6章，[文本摘要](chapter6.md)

挖掘了文本总结的复杂的序列-序列任务，并探讨了用于这一任务的指标。 

##  第七章  [问答系统](chapter7.md)

主要是建立一个基于评论的问题回答系统，并介绍了用Haystack进行检索的情况。 

## 第8章 [Transformers产品级优化](chapter8.md)

重点是模型性能。 我们将研究意图检测的任务（一种序列分类问题），并探索知识提炼、量化和修剪等技术。

## 第9章，[处理少量标签或无标签问题](chapter9.md)

探讨了在没有大量标签数据的情况下提高模型性能的方法。 我们将建立一个GitHub问题标注器，并探索诸如零样本分类和数据增强等技术。



 ## 第10章，[从零开始训练Transformers](chapter10.md)

告诉你如何从零开始建立和训练一个用于自动完成Python源代码的模型。 我们将研究数据集流和大规模训练，并建立我们自己的标记器。 

## 第11章，[未来的发展方向](chapter11.md)

探讨了Transformers所面临的挑战和该研究领域的一些令人兴奋的新方向。

## [致谢](thanks.md) [done]
